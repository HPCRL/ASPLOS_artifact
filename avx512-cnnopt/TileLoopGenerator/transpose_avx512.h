#pragma once
#include <stdio.h>
#include "immintrin.h"

void transpose16x16_avx512(float* mat, float* matT, const int lda, const int ldb) {
    int i,j;

    __m512 t0, t1, t2, t3, t4, t5, t6, t7, t8, t9, ta, tb, tc, td, te, tf;
    __m512 r0, r1, r2, r3, r4, r5, r6, r7, r8, r9, ra, rb, rc, rd, re, rf;

    r0 = _mm512_load_ps(&mat[ 0*lda]);
    r1 = _mm512_load_ps(&mat[ 1*lda]);
    r2 = _mm512_load_ps(&mat[ 2*lda]);
    r3 = _mm512_load_ps(&mat[ 3*lda]);
    r4 = _mm512_load_ps(&mat[ 4*lda]);
    r5 = _mm512_load_ps(&mat[ 5*lda]);
    r6 = _mm512_load_ps(&mat[ 6*lda]);
    r7 = _mm512_load_ps(&mat[ 7*lda]);
    r8 = _mm512_load_ps(&mat[ 8*lda]);
    r9 = _mm512_load_ps(&mat[ 9*lda]);
    ra = _mm512_load_ps(&mat[10*lda]);
    rb = _mm512_load_ps(&mat[11*lda]);
    rc = _mm512_load_ps(&mat[12*lda]);
    rd = _mm512_load_ps(&mat[13*lda]);
    re = _mm512_load_ps(&mat[14*lda]);
    rf = _mm512_load_ps(&mat[15*lda]);

    t0 = _mm512_unpacklo_ps(r0,r1); //   0  16   1  17   4  20   5  21   8  24   9  25  12  28  13  29 
    t1 = _mm512_unpackhi_ps(r0,r1); //   2  18   3  19   6  22   7  23  10  26  11  27  14  30  15  31
    t2 = _mm512_unpacklo_ps(r2,r3); //  32  48  33  49 ...
    t3 = _mm512_unpackhi_ps(r2,r3); //  34  50  35  51 ...
    t4 = _mm512_unpacklo_ps(r4,r5); //  64  80  65  81 ...  
    t5 = _mm512_unpackhi_ps(r4,r5); //  66  82  67  83 ...
    t6 = _mm512_unpacklo_ps(r6,r7); //  96 112  97 113 ...
    t7 = _mm512_unpackhi_ps(r6,r7); //  98 114  99 115 ...
    t8 = _mm512_unpacklo_ps(r8,r9); // 128 ...
    t9 = _mm512_unpackhi_ps(r8,r9); // 130 ...
    ta = _mm512_unpacklo_ps(ra,rb); // 160 ...
    tb = _mm512_unpackhi_ps(ra,rb); // 162 ...
    tc = _mm512_unpacklo_ps(rc,rd); // 196 ...
    td = _mm512_unpackhi_ps(rc,rd); // 198 ...
    te = _mm512_unpacklo_ps(re,rf); // 228 ...
    tf = _mm512_unpackhi_ps(re,rf); // 230 ...

    r0 = _mm512_unpacklo_pd(t0,t2); //   0  16  32  48 ...
    r1 = _mm512_unpackhi_pd(t0,t2); //   1  17  33  49 ...
    r2 = _mm512_unpacklo_pd(t1,t3); //   2  18  34  49 ...
    r3 = _mm512_unpackhi_pd(t1,t3); //   3  19  35  51 ...
    r4 = _mm512_unpacklo_pd(t4,t6); //  64  80  96 112 ...  
    r5 = _mm512_unpackhi_pd(t4,t6); //  65  81  97 114 ...
    r6 = _mm512_unpacklo_pd(t5,t7); //  66  82  98 113 ...
    r7 = _mm512_unpackhi_pd(t5,t7); //  67  83  99 115 ...
    r8 = _mm512_unpacklo_pd(t8,ta); // 128 144 160 176 ...  
    r9 = _mm512_unpackhi_pd(t8,ta); // 129 145 161 178 ...
    ra = _mm512_unpacklo_pd(t9,tb); // 130 146 162 177 ... 
    rb = _mm512_unpackhi_pd(t9,tb); // 131 147 163 179 ...
    rc = _mm512_unpacklo_pd(tc,te); // 192 208 228 240 ... 
    rd = _mm512_unpackhi_pd(tc,te); // 193 209 229 241 ...
    re = _mm512_unpacklo_pd(td,tf); // 194 210 230 242 ...
    rf = _mm512_unpackhi_pd(td,tf); // 195 211 231 243 ...

    t0 = _mm512_shuffle_f32x4(r0, r4, 0x88); //   0  16  32  48   8  24  40  56  64  80  96  112 ...
    t1 = _mm512_shuffle_f32x4(r1, r5, 0x88); //   1  17  33  49 ...
    t2 = _mm512_shuffle_f32x4(r2, r6, 0x88); //   2  18  34  50 ...
    t3 = _mm512_shuffle_f32x4(r3, r7, 0x88); //   3  19  35  51 ...
    t4 = _mm512_shuffle_f32x4(r0, r4, 0xdd); //   4  20  36  52 ...
    t5 = _mm512_shuffle_f32x4(r1, r5, 0xdd); //   5  21  37  53 ...
    t6 = _mm512_shuffle_f32x4(r2, r6, 0xdd); //   6  22  38  54 ...
    t7 = _mm512_shuffle_f32x4(r3, r7, 0xdd); //   7  23  39  55 ...
    t8 = _mm512_shuffle_f32x4(r8, rc, 0x88); // 128 144 160 176 ...
    t9 = _mm512_shuffle_f32x4(r9, rd, 0x88); // 129 145 161 177 ...
    ta = _mm512_shuffle_f32x4(ra, re, 0x88); // 130 146 162 178 ...
    tb = _mm512_shuffle_f32x4(rb, rf, 0x88); // 131 147 163 179 ...
    tc = _mm512_shuffle_f32x4(r8, rc, 0xdd); // 132 148 164 180 ...
    td = _mm512_shuffle_f32x4(r9, rd, 0xdd); // 133 149 165 181 ...
    te = _mm512_shuffle_f32x4(ra, re, 0xdd); // 134 150 166 182 ...
    tf = _mm512_shuffle_f32x4(rb, rf, 0xdd); // 135 151 167 183 ...

    r0 = _mm512_shuffle_f32x4(t0, t8, 0x88); //   0  16  32  48  64  80  96 112 ... 240
    r1 = _mm512_shuffle_f32x4(t1, t9, 0x88); //   1  17  33  49  66  81  97 113 ... 241
    r2 = _mm512_shuffle_f32x4(t2, ta, 0x88); //   2  18  34  50  67  82  98 114 ... 242
    r3 = _mm512_shuffle_f32x4(t3, tb, 0x88); //   3  19  35  51  68  83  99 115 ... 243
    r4 = _mm512_shuffle_f32x4(t4, tc, 0x88); //   4 ...
    r5 = _mm512_shuffle_f32x4(t5, td, 0x88); //   5 ...
    r6 = _mm512_shuffle_f32x4(t6, te, 0x88); //   6 ...
    r7 = _mm512_shuffle_f32x4(t7, tf, 0x88); //   7 ...
    r8 = _mm512_shuffle_f32x4(t0, t8, 0xdd); //   8 ...
    r9 = _mm512_shuffle_f32x4(t1, t9, 0xdd); //   9 ...
    ra = _mm512_shuffle_f32x4(t2, ta, 0xdd); //  10 ...
    rb = _mm512_shuffle_f32x4(t3, tb, 0xdd); //  11 ...
    rc = _mm512_shuffle_f32x4(t4, tc, 0xdd); //  12 ...
    rd = _mm512_shuffle_f32x4(t5, td, 0xdd); //  13 ...
    re = _mm512_shuffle_f32x4(t6, te, 0xdd); //  14 ...
    rf = _mm512_shuffle_f32x4(t7, tf, 0xdd); //  15  31  47  63  79  96 111 127 ... 255
    
    _mm512_store_epi32(&matT[ 0*ldb], r0);
    _mm512_store_epi32(&matT[ 1*ldb], r1);
    _mm512_store_epi32(&matT[ 2*ldb], r2);
    _mm512_store_epi32(&matT[ 3*ldb], r3);
    _mm512_store_epi32(&matT[ 4*ldb], r4);
    _mm512_store_epi32(&matT[ 5*ldb], r5);
    _mm512_store_epi32(&matT[ 6*ldb], r6);
    _mm512_store_epi32(&matT[ 7*ldb], r7);
    _mm512_store_epi32(&matT[ 8*ldb], r8);
    _mm512_store_epi32(&matT[ 9*ldb], r9);
    _mm512_store_epi32(&matT[10*ldb], ra);
    _mm512_store_epi32(&matT[11*ldb], rb);
    _mm512_store_epi32(&matT[12*ldb], rc);
    _mm512_store_epi32(&matT[13*ldb], rd);
    _mm512_store_epi32(&matT[14*ldb], re);
    _mm512_store_epi32(&matT[15*ldb], rf);
}
